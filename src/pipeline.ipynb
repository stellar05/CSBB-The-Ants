{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa13fd1",
   "metadata": {},
   "source": [
    "## Lymph Node Metastasis Detection Pipeline\n",
    "\n",
    "Welcome to the notebook for The Ants machine learning pipeline. This notebook contains a simple machine learning model trained using images from the [PCam](https://github.com/basveeling/pcam) dataset. This model was used to extract areas of interest to image from patient lymph node slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from PIL import ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f096258",
   "metadata": {},
   "source": [
    "## Custom H5 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5PatchDataset(Dataset):\n",
    "    \"\"\"Reads images/labels from an .h5 file. Assumes datasets 'x' and 'y' exist.\n",
    "    x: (N, H, W, C) uint8\n",
    "    y: (N,) int labels 0/1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, h5_path_x, h5_path_y, transform=None, img_key=\"x\", label_key=\"y\"\n",
    "    ):\n",
    "        self.h5_path_x = h5_path_x\n",
    "        self.h5_path_y = h5_path_y\n",
    "        self.transform = transform\n",
    "        self.img_key = img_key\n",
    "        self.label_key = label_key\n",
    "\n",
    "        self._h5_img = None\n",
    "        self._h5_label = None\n",
    "        self._length = None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._length is not None:\n",
    "            return int(self._length)\n",
    "        with h5py.File(self.h5_path_y, \"r\") as fl:\n",
    "            return int(fl[self.label_key].shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self._h5_img is None:\n",
    "            self._h5_img = h5py.File(self.h5_path_x, \"r\")\n",
    "            self._h5_label = h5py.File(self.h5_path_y, \"r\")\n",
    "            self._length = int(self._h5_label[self.label_key].shape[0])\n",
    "\n",
    "        try:\n",
    "            img = self._h5_img[self.img_key][idx]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed reading image index {idx} with key '{self.img_key}'\"\n",
    "            ) from e\n",
    "        try:\n",
    "            label = int(self._h5_label[self.label_key][idx])\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed reading label index {idx} with key '{self.label_key}'\"\n",
    "            ) from e\n",
    "\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            img = np.array(img)\n",
    "\n",
    "        pil = Image.fromarray(img.astype(\"uint8\"))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img_tensor = self.transform(pil)\n",
    "        else:\n",
    "            img_tensor = T.ToTensor()(pil)\n",
    "\n",
    "        return img_tensor, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c208eb",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "The code below creates a convolutional neural network to work with the PCam 96 x 96 images. It consists of three convolutional layers with relu activation function and batch normalization, with average pooling between layers. A fully connected neural network is used to achieve the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db309e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_features):\n",
    "        super(CNN, self).__init__()\n",
    "        # 96 x 96\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, hidden_channels[0], kernel_size=3, padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_channels[0])\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.max_pool1 = nn.AvgPool2d(2)\n",
    "        # 48 x 48\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            hidden_channels[0], hidden_channels[1], kernel_size=5, padding=2\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_channels[0])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.AvgPool2d(2)\n",
    "        # 24 x 24\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            hidden_channels[1], hidden_channels[2], kernel_size=5, padding=2\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.max_pool3 = nn.AvgPool2d(2)\n",
    "        # 12 x 12\n",
    "        self.fc1 = nn.Linear(12 * 12 * hidden_channels[2], 12 * 12)\n",
    "        self.fc = nn.Linear(144, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1912e24",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(img_size=96, train=True):\n",
    "    if train:\n",
    "        return T.Compose(\n",
    "            [\n",
    "                T.Resize((img_size, img_size)),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip(),\n",
    "                T.RandomRotation(90),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return T.Compose(\n",
    "            [\n",
    "                T.Resize((img_size, img_size)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "def eval_accuracy(data_loader, cnn, device=torch.device(\"cpu\")):\n",
    "    cnn.eval()\n",
    "\n",
    "    accuracy = 0\n",
    "    n = 0\n",
    "\n",
    "    for X, y in tqdm(data_loader, desc=\"evaluation\"):\n",
    "        X, y = X.to(device), y.to(device).long()\n",
    "        with torch.no_grad():\n",
    "            preds = ((cnn(X).squeeze(1)) > 0).long()\n",
    "            accuracy += (preds == y).sum().item()\n",
    "            n += y.size(0)\n",
    "\n",
    "    return accuracy / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f66bba",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Run the code below to load the PCam dataset into dataloaders. The code will automatically select `cuda` if available, otherwise the CPU will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffce544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "h5_img = \"../data/camelyonpatch_split_training_x.h5\"\n",
    "h5_label = \"../data/camelyonpatch_split_training_y.h5\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "transform_train = get_transforms()\n",
    "transform_test = get_transforms(train=False)\n",
    "\n",
    "train_data = H5PatchDataset(\n",
    "    h5_path_x=h5_img,\n",
    "    h5_path_y=h5_label,\n",
    "    transform=transform_train,\n",
    ")\n",
    "test_data = H5PatchDataset(\n",
    "    h5_path_x=h5_img,\n",
    "    h5_path_y=h5_label,\n",
    "    transform=transform_test,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3558a",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "In the code below you can select the model you would like to use. The first model is a custom model that can be adjusted in the `CNN` class and has parameters for input channels, hidden channels, and output features that can be adjusted as desired. The second is the pretrained `resnet18` model optimized for image recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use our custom model:\n",
    "in_channels = 3\n",
    "hidden_channels = [64, 64, 64]\n",
    "out_features = 1\n",
    "\n",
    "# cnn = CNN(in_channels, hidden_channels, out_features)\n",
    "# optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Uncomment to use resnet18 (pretrained model)\n",
    "from torchvision import models\n",
    "\n",
    "cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "cnn.fc = nn.Linear(cnn.fc.in_features, 1)\n",
    "optimizer = torch.optim.AdamW(cnn.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82057bc",
   "metadata": {},
   "source": [
    "## Training & Evaluation\n",
    "\n",
    "The code block below trains the model using pyTorch for a specified number of epochs. Adjust the value of `epochs` as desired. Accuracy for the training set is stored in `train_acc` and for the test set is stored in `test_acc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cnn.train()\n",
    "    cnn.to(device)\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(\n",
    "        tqdm(train_loader, desc=\"train\", leave=False)\n",
    "    ):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = cnn(x_batch)\n",
    "\n",
    "        loss = criterion(y_pred.squeeze(1), y_batch)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_accuracy = 100 * eval_accuracy(train_loader, cnn.to(\"cpu\"))\n",
    "    test_accuracy = 100 * eval_accuracy(test_loader, cnn.to(\"cpu\"))\n",
    "\n",
    "    train_acc.append(train_accuracy)\n",
    "    test_acc.append(test_accuracy)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    print(\"Train accuracy: {:.00f}%\".format(train_accuracy))\n",
    "    print(\"Test accuracy: {:.00f}%\".format(test_accuracy))\n",
    "\n",
    "\n",
    "print(train_acc)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31268cc3",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "If you would like to save the model so that it can be loaded again later, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aebe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../results_notebook\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(output_dir, \"best.pth\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state\": cnn.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    },\n",
    "    save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ef62c",
   "metadata": {},
   "source": [
    "## TIFF Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d392281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiffPatchDataset(Dataset):\n",
    "    def __init__(self, pathes, patch_size, transforms):\n",
    "        self.tiff_pathes = pathes\n",
    "        self.patch_size = patch_size\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.index = []\n",
    "        self.image_sizes = []\n",
    "\n",
    "        for i, f in enumerate(self.tiff_pathes):\n",
    "            with Image.open(f) as img:\n",
    "                w, h = img.size\n",
    "            self.image_sizes.append((w, h))\n",
    "\n",
    "            x_vals = range(0, w - self.patch_size + 1, self.patch_size)\n",
    "            y_vals = range(0, h - self.patch_size + 1, self.patch_size)\n",
    "\n",
    "            for x in x_vals:\n",
    "                for y in y_vals:\n",
    "                    self.index.append((i, x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, x, y = self.index[index]\n",
    "        x2 = x + self.patch_size\n",
    "        y2 = y + self.patch_size\n",
    "\n",
    "        tiff_path = self.tiff_pathes[image]\n",
    "        img = Image.open(tiff_path)\n",
    "        w, h = img.size\n",
    "\n",
    "        try:\n",
    "            img = img.convert(\"RGB\")\n",
    "        except Exception:\n",
    "            print(\"Issue with RGB conversion\")\n",
    "\n",
    "        if x2 <= w and y2 <= h:\n",
    "            patch = img.crop((x, y, x2, y2))\n",
    "\n",
    "        patch_trans = self.transforms(patch)\n",
    "\n",
    "        meta = {\n",
    "            \"image_path\": self.tiff_pathes[image],\n",
    "            \"x\": int(x),\n",
    "            \"y\": int(y),\n",
    "            \"patch_w\": self.patch_size,\n",
    "            \"patch_h\": self.patch_size,\n",
    "            \"img_idx\": int(image),\n",
    "        }\n",
    "        return patch_trans, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = [b[0] for b in batch]\n",
    "    metas = [b[1] for b in batch]\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    return imgs, metas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7b281",
   "metadata": {},
   "source": [
    "## Using the Model on TIFF Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"../results_notebook/tiff_preds_notebook_resnet/best.pth\"\n",
    "save_path = \"../results_notebook/tiff_preds_notebook/best.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "in_channels = 3\n",
    "hidden_channels = [64, 64, 64]\n",
    "out_features = 1\n",
    "\n",
    "# Uncomment to use our custom model:\n",
    "model = CNN(in_channels, hidden_channels, out_features)\n",
    "\n",
    "# Uncomment to use resnet18 (pretrained model)\n",
    "# from torchvision import models\n",
    "# model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "model.load_state_dict(torch.load(save_path)[\"model_state\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_dir = \"../tiff_files\"\n",
    "tiff_paths = sorted(\n",
    "    glob(os.path.join(tiff_dir, \"**\", \"*.tif\"), recursive=True)\n",
    "    + glob(os.path.join(tiff_dir, \"**\", \"*.tiff\"), recursive=True)\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = get_transforms(img_size=96, train=False)\n",
    "\n",
    "dataset = TiffPatchDataset(tiff_paths, 96, transform)\n",
    "print(f\"Total patches: {len(dataset)}\")\n",
    "print(dataset.__getitem__(1)[0].shape)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4, collate_fn=collate_fn, pin_memory=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "\n",
    "output_dir = \"../results_notebook/tiff_preds_notebook\"\n",
    "output_csv = \"patches_preds\"\n",
    "threshold = 0.7\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_path = os.path.join(output_dir, output_csv)\n",
    "\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for imgs, metas in tqdm(dataloader, desc=\"inference\"):\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs).squeeze(1)\n",
    "\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy().tolist()\n",
    "        preds = [1 if p >= threshold else 0 for p in probs]\n",
    "\n",
    "        # debug\n",
    "        # print(f\"Outputs: {outputs}\")\n",
    "        # print(f\"Probs: {probs}\")\n",
    "        # print(f\"Preds: {preds}\")\n",
    "        for m, p, pr in zip(metas, probs, preds):\n",
    "            results.append(\n",
    "                {\n",
    "                    \"image_path\": m[\"image_path\"],\n",
    "                    \"x\": m[\"x\"],\n",
    "                    \"y\": m[\"y\"],\n",
    "                    \"probability\": float(p),\n",
    "                    \"prediction\": int(pr),\n",
    "                }\n",
    "            )\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"image_path\", \"x\", \"y\", \"probability\", \"prediction\"])\n",
    "    for r in results:\n",
    "        writer.writerow(\n",
    "            [r[\"image_path\"], r[\"x\"], r[\"y\"], r[\"probability\"], r[\"prediction\"]]\n",
    "        )\n",
    "\n",
    "print(\"Saved per-patch predictions to:\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78fb4f8",
   "metadata": {},
   "source": [
    "## Visualize Outputs\n",
    "The code below takes the CSV file outputted from the above prediction code and highlights the patches where the model predicted the presence of metastases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4856d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 96\n",
    "font_size = 30\n",
    "\n",
    "\n",
    "def draw_box(img_path, boxes, out_path, width=3):\n",
    "    im = Image.open(img_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font = ImageFont.truetype(\"../arial.ttf\", font_size)\n",
    "\n",
    "    # font = ImageFont.load_default()\n",
    "\n",
    "    for b in boxes:\n",
    "        x = int(b[\"x\"])\n",
    "        y = int(b[\"y\"])\n",
    "        p = b.get(\"probability\", None)\n",
    "        draw_color = (\n",
    "            (255, 255, 0) if p < 0.8 else (255, 165, 0) if p < 0.9 else (255, 0, 0)\n",
    "        )\n",
    "\n",
    "        rect = (x, y, x + patch_size, y + patch_size)\n",
    "\n",
    "        for i in range(width):\n",
    "            draw.rectangle(\n",
    "                (rect[0] - i, rect[1] - i, rect[2] + i, rect[3] + i),\n",
    "                outline=draw_color,\n",
    "            )\n",
    "\n",
    "        text = f\"{p:.2f}\"\n",
    "        text_pos = (x + (patch_size // 2) - 20, y + (patch_size // 2) - 10)\n",
    "        draw.text(text_pos, text, fill=draw_color, font=font)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    im.save(out_path)\n",
    "    print(\"Saved:\", out_path)\n",
    "    display(im)\n",
    "\n",
    "\n",
    "output_dir = \"../results_notebook/tiff_preds_notebook\"\n",
    "output_csv = \"patches_preds\"\n",
    "csv_path = os.path.join(output_dir, output_csv)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "grouped = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    if int(row[\"prediction\"]) == 1:\n",
    "        grouped[row[\"image_path\"]].append(\n",
    "            {\n",
    "                \"x\": int(row[\"x\"]),\n",
    "                \"y\": int(row[\"y\"]),\n",
    "                \"probability\": float(row[\"probability\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"{len(grouped)} files with positive patches found\")\n",
    "\n",
    "for img_path, boxes in grouped.items():\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"WARNING: image not found: {img_path} â€” skipping\")\n",
    "        continue\n",
    "    base = os.path.basename(img_path)\n",
    "    out_path = os.path.join(\n",
    "        output_dir, base.replace(\".tif\", \"_annot.png\").replace(\".tiff\", \"_annot.png\")\n",
    "    )\n",
    "    draw_box(img_path, boxes, out_path, width=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
